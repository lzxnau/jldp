# Transformer

## Word Embedding

Embedding dim
: The dimensionality of the vector representation for each word/token.
  This determines how much information can be encoded in that single vector.

Hidden dim
: The dimensionality of the hidden states within the transformer's attention
  layers. This represents the model's internal working memory and capacity to
  process relationships between words in the sequence.

## Positional Encoding

## Multi-Head Attention

## Masked Multi-Head Attention

## Residual Connection

## Layer Normalization

## Positionwise Feed-Forward Network
