# Transformer

## Word Embedding

Embedding dim
: The dimensionality of the vector representation for each word/token.
  This determines how much information can be encoded in that single vector.

Hidden dim
: The dimensionality of the hidden states within the transformer's attention
  layers. This represents the model's internal working memory and capacity to
  process relationships between words in the sequence.

Embeddings start with relatively lower dimensionality and are projected into a
higher-dimensional space for computation within the attention mechanisms.

## Positional Encoding

## Multi-Head Attention

## Masked Multi-Head Attention

## Residual Connection

## Layer Normalization

## Positionwise Feed-Forward Network
